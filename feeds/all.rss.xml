<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Momo Ong</title><link>/</link><description></description><atom:link href="/feeds/all.rss.xml" rel="self"></atom:link><lastBuildDate>Tue, 08 Apr 2014 23:00:00 -0400</lastBuildDate><item><title>[ML Notes] - Naive Bayes</title><link>/naive-bayes.html</link><description>&lt;h3&gt;The Classification Problem&lt;/h3&gt;
&lt;p&gt;Let $C$ be a class, and $F_{i1}, F_{i2}, \ldots, F_{in}$ be the $n$ features in the $i$th instance. Given observed features, the probability that the $i$th instance belongs to class $C$ is simply&lt;/p&gt;
&lt;p&gt;$$ P(C|F_{i1}, F_{i2}, \ldots, F_{in})$$&lt;/p&gt;
&lt;p&gt;Thus, the classification problem seeks to determine the following&lt;/p&gt;
&lt;p&gt;$$\underset{c} {\mathrm{argmax}} P(C|F_{i1}, F_{i2}, \ldots, F_{in})$$&lt;/p&gt;
&lt;hr /&gt;
&lt;h3&gt;The Naive Bayes Classifier&lt;/h3&gt;
&lt;p&gt;Using Bayes' Theorem, we can know that the above conditional probability can be written as&lt;/p&gt;
&lt;p&gt;$$ P(C|F_{i1}, F_{i2}, \ldots, F_{in}) = \frac{P(C)P(F_{i1}, F_{i2}, \ldots, F_{in}|C)}{P(F_{i1}, F_{i2}, \ldots, F_{in})}$$&lt;/p&gt;
&lt;p&gt;Now for a few definitions to better understand the terms in the above equation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$P(C)$ is the &lt;em&gt;a priori&lt;/em&gt;, or &lt;em&gt;prior&lt;/em&gt; probability of the class $C$. This is the probability that an instance belongs to class $C$, &lt;em&gt;prior&lt;/em&gt; to knowing anything about the instance's features. Suppose there were 2 blue balls and 5 red balls. The &lt;em&gt;a priori&lt;/em&gt; probability of blue balls and red balls would be $2/7$ and $5/7$ respectively.&lt;/li&gt;
&lt;li&gt;$P(C|F_{i1}, F_{i2}, \ldots, F_{in})$ is termed the &lt;em&gt;a posteriori&lt;/em&gt; probability of class $C$. This is the probability that an instance belongs to class $C$, &lt;em&gt;given&lt;/em&gt; knowledge of the instance's features.&lt;/li&gt;
&lt;li&gt;$P(F_{i1}, F_{i2}, \ldots, F_{in}|C)$ can be seen as the &lt;em&gt;likelihood&lt;/em&gt; function of $C$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, we assume the independence of all features to each other given the class $C$. This is the naive part of the classifier that gives it its name and allows us to rewrite the above Bayesian equation as&lt;/p&gt;
&lt;p&gt;$$ P(C|F_{i1}, F_{i2}, \ldots, F_{in}) = \frac{P(C) P(F_{i1}|C) P(F_{i2}|C) \ldots P(F_{in}|C)}{P(F_{i1}, F_{i2}, \ldots, F_{in})} $$&lt;/p&gt;
&lt;p&gt;Since we are not interested in the absolute value of $P(C|F_{i1}, F_{i2}, \ldots, F_{in})$, the denominator of the above equation does not matter, and the Naive Bayes classifier algorithm can be understood to calculate&lt;/p&gt;
&lt;p&gt;$$ P(C) \underset{c} {\mathrm{argmax}} \prod_i P(F_i|C) $$&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Sources&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://code.google.com/p/ourmine/wiki/LectureNaiveBayes"&gt;Ourmine NaiveBayes Classifiers 101&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stanford.edu/class/cs124/lec/naivebayes.pdf"&gt;Stanford CS124 Lectures&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Momo Ong</dc:creator><pubDate>Tue, 08 Apr 2014 23:00:00 -0400</pubDate><guid>tag:,2014-04-08:naive-bayes.html</guid><category>data science</category><category>machine learning</category><category>notes</category></item><item><title>[ML Notes] - Preface</title><link>/machine-learning-notes.html</link><description>&lt;p&gt;I've started taking notes on Machine Learning and Data Science in general to help cement my understanding of concepts. I hope they will be of use to my readers, as well.&lt;/p&gt;
&lt;p&gt;I have tried my best to give credit where due, but as these are just fast, brief notes, my citations are by no means complete.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Momo Ong</dc:creator><pubDate>Mon, 07 Apr 2014 00:30:00 -0400</pubDate><guid>tag:,2014-04-07:machine-learning-notes.html</guid><category>data science</category><category>machine learning</category><category>notes</category></item><item><title>Hello World!</title><link>/hello-world.html</link><description>&lt;p&gt;Hello World! I created this blog/website with the &lt;code&gt;Pelican&lt;/code&gt; static site generator, using the beautiful &lt;code&gt;flasky&lt;/code&gt; theme by &lt;a href="http://fjavieralba.com/"&gt;fjavieralba&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On this blog, I will not only express my (techncial &lt;em&gt;and&lt;/em&gt; non-technical) thoughts, but also document interesting concepts I come across while working with data.&lt;/p&gt;
&lt;p&gt;I hope you will enjoy the content of this blog!&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Momo Ong</dc:creator><pubDate>Sun, 30 Mar 2014 21:30:00 -0400</pubDate><guid>tag:,2014-03-30:hello-world.html</guid><category>blog</category></item></channel></rss>